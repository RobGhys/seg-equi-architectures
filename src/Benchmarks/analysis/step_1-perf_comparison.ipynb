{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of the results",
   "id": "971a73f49bc12cd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.421386Z",
     "start_time": "2024-09-06T13:21:14.417664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats"
   ],
   "id": "7749ec064416a5a8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get best results for each fold",
   "id": "30c7f6ffe19fe13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.470259Z",
     "start_time": "2024-09-06T13:21:14.465778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def construct_path_from_parent(root_dir, subdir_list):\n",
    "    # Construct the path using os.path.join\n",
    "    return os.path.join(root_dir, *subdir_list)"
   ],
   "id": "bb8cd9eb1fc259f1",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.518184Z",
     "start_time": "2024-09-06T13:21:14.513079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "root_dir = '/home/rob/Documents/3_projects/bench/_results/small_models'\n",
    "output_dir_base_path = '/home/rob/Documents/3_projects/bench/analysis/small_models'\n",
    "os.makedirs(output_dir_base_path, exist_ok=True)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "#dataset_name = \"kvasir\"\n",
    "#dataset_name = \"NucleiSeg\"\n",
    "#dataset_name = \"URDE\"\n",
    "dataset_name = \"isaid\"\n",
    "#dataset_name = \"coco\"\n",
    "\n",
    "model_names = [\"UNet_vanilla\", \"UNet_bcnn\", \"UNet_e2cnn\"]\n",
    "\n",
    "folds = [f'fold_{i}' for i in range(5)]\n",
    "criteria = 'loss_dice'\n",
    "\n",
    "if dataset_name in [\"kvasir\", \"NucleiSeg\", \"URDE\"]:\n",
    "    perf_metrics = ['dice_score', 'IoU_score', 'precision_metric', 'recall_metric', 'accuracy_metric']\n",
    "else:\n",
    "    perf_metrics = ['IoU_score', 'precision_metric', 'recall_metric', 'accuracy_metric', 'average_precision']"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.568400Z",
     "start_time": "2024-09-06T13:21:14.562154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_best_metrics(summary_loc: str, fold_nb: int, result_dict: dict, \n",
    "                     perf_metrics: list, criteria: str, num_params: int) -> int:\n",
    "    with open(summary_loc) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    if num_params == 0:\n",
    "        num_params = data.get('n_params', 0)\n",
    "    data = data.get('test', {})\n",
    "\n",
    "    if criteria not in data or not data[criteria]:\n",
    "        raise ValueError(f\"Criteria '{criteria}' is not in the data or has no values.\")\n",
    "    \n",
    "    best_idx = np.argmin(data[criteria])\n",
    "    \n",
    "    if best_idx >= len(data[criteria]):\n",
    "        raise IndexError(\"Calculated best index is out of bounds for the criteria list.\")\n",
    "    \n",
    "    if not all(len(data[metric]) == len(data[criteria]) for metric in perf_metrics):\n",
    "        raise ValueError(\"Mismatch in length between criteria and performance metrics.\")\n",
    "    \n",
    "    fold_dict: dict = {}\n",
    "    for metric in perf_metrics:\n",
    "        fold_dict[metric] = data[metric][best_idx]\n",
    "    fold_dict['epoch'] = best_idx\n",
    "    \n",
    "    if fold_nb in result_dict:\n",
    "        raise KeyError(f\"Key '{fold_nb}' already present in dictionary.\")\n",
    "    \n",
    "    result_dict[fold_nb] = fold_dict\n",
    "    \n",
    "    return num_params\n"
   ],
   "id": "aba3c3ffe095f3b5",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.617615Z",
     "start_time": "2024-09-06T13:21:14.613047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_path(dataset_name: str, \n",
    "               model_name: str, \n",
    "               fold_nb: str, \n",
    "               root_dir: str = '/home/rob/Documents/3_projects/bench/_results') -> str:\n",
    "    subdirs = [dataset_name, model_name, fold_nb, \"summary.json\"]\n",
    "\n",
    "    summary_loc = construct_path_from_parent(root_dir, subdirs)\n",
    "    if os.path.exists(summary_loc):\n",
    "        print(f'summary.json loc: {summary_loc}')\n",
    "    else:\n",
    "        print(f'No file at the path you provided: {summary_loc}.')\n",
    "    \n",
    "    return summary_loc"
   ],
   "id": "60490ea8f8330b37",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T13:21:14.699057Z",
     "start_time": "2024-09-06T13:21:14.662429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model_name in model_names:\n",
    "    result_dict: dict = {}\n",
    "    num_params: int = 0\n",
    "    \n",
    "    for fold in folds:\n",
    "        summary_loc = build_path(dataset_name, model_name, fold, root_dir=root_dir)\n",
    "        num_params = get_best_metrics(summary_loc, fold, result_dict, perf_metrics, criteria, num_params)\n",
    "        \n",
    "    results = {'num_params': num_params}\n",
    "\n",
    "    for metric in perf_metrics:\n",
    "        data = np.array([result_dict[f\"fold_{i}\"][metric] for i in range(5)])\n",
    "        mean = np.mean(data)\n",
    "        ci = stats.t.interval(confidence=0.95, df=len(data)-1, loc=np.mean(data), scale=stats.sem(data))\n",
    "        results[metric] = {\n",
    "            \"mean\": mean,\n",
    "            \"95% CI\": ci\n",
    "        }\n",
    "        \n",
    "    output_path = os.path.join(output_dir_base_path, dataset_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    json_name = f'/{dataset_name}_{model_name}_data.json'\n",
    "    with open(f'{output_path}{json_name}', 'w') as fp:\n",
    "        json.dump(results, fp, default=str, indent=4)\n",
    "        \n",
    "        "
   ],
   "id": "2ad9b65c9343b8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file at the path you provided: /home/rob/Documents/3_projects/bench/_results/small_models/isaid/UNet_vanilla/fold_0/summary.json.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/rob/Documents/3_projects/bench/_results/small_models/isaid/UNet_vanilla/fold_0/summary.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold \u001B[38;5;129;01min\u001B[39;00m folds:\n\u001B[1;32m      6\u001B[0m     summary_loc \u001B[38;5;241m=\u001B[39m build_path(dataset_name, model_name, fold, root_dir\u001B[38;5;241m=\u001B[39mroot_dir)\n\u001B[0;32m----> 7\u001B[0m     num_params \u001B[38;5;241m=\u001B[39m \u001B[43mget_best_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43msummary_loc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mperf_metrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriteria\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m results \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_params\u001B[39m\u001B[38;5;124m'\u001B[39m: num_params}\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric \u001B[38;5;129;01min\u001B[39;00m perf_metrics:\n",
      "Cell \u001B[0;32mIn[39], line 3\u001B[0m, in \u001B[0;36mget_best_metrics\u001B[0;34m(summary_loc, fold_nb, result_dict, perf_metrics, criteria, num_params)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_best_metrics\u001B[39m(summary_loc: \u001B[38;5;28mstr\u001B[39m, fold_nb: \u001B[38;5;28mint\u001B[39m, result_dict: \u001B[38;5;28mdict\u001B[39m, \n\u001B[1;32m      2\u001B[0m                      perf_metrics: \u001B[38;5;28mlist\u001B[39m, criteria: \u001B[38;5;28mstr\u001B[39m, num_params: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msummary_loc\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m      4\u001B[0m         data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_params \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/Documents/Github/seg-equi-architectures/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[0;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/rob/Documents/3_projects/bench/_results/small_models/isaid/UNet_vanilla/fold_0/summary.json'"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2bcdd5d162aca6f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
